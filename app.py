import streamlit as st
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from datetime import timedelta
import io
import re

# --- ç¶²é è¨­å®š ---
st.set_page_config(page_title="CRM æ™ºæ…§ç”¢å“é æ¸¬ç³»çµ±", page_icon="ğŸ§ ", layout="wide")

# ==========================================
# ğŸ§  æ ¸å¿ƒå‡ç´š: æ™ºæ…§æ¬„ä½å°ç…§è¡¨
# ==========================================
COLUMN_MAPPING = {
    'date': ['å–®æ“šæ—¥æœŸ', 'ä¸‹å–®æ—¥', 'æ—¥æœŸ', 'éŠ·è²¨æ—¥æœŸ', 'äº¤æ˜“æ—¥æœŸ', 'è¨‚å–®æ—¥æœŸ', 'Date', 'Order Date'],
    'qty': ['æ•¸é‡', 'è¨‚å–®æ•¸é‡', 'éŠ·è²¨æ•¸é‡', 'Qty', 'Quantity', 'Amount'],
    'product': ['ç”¢å“ç·¨è™Ÿ', 'å“è™Ÿ', 'å“å', 'æ–™è™Ÿ', 'Product ID'],
    'customer': ['å®¢æˆ¶', 'å®¢æˆ¶ä»£è™Ÿ', 'å®¢æˆ¶ç°¡ç¨±', 'å®¢æˆ¶åç¨±', 'Customer']
}

def find_column(df, target_type):
    candidates = COLUMN_MAPPING.get(target_type, [])
    for col in df.columns:
        if str(col).strip() in candidates: return col
    for col in df.columns:
        for candidate in candidates:
            if candidate in str(col): return col
    return None

# ==========================================
# ğŸ§¹ ERP è³‡æ–™æ¸…æ´—æ ¸å¿ƒ (v8 å¼·åŠ›ç‰ˆ)
# ==========================================
def try_read_content(uploaded_file):
    """
    æš´åŠ›å˜—è©¦è®€å–æª”æ¡ˆå…§å®¹ï¼Œè§£æ±º Big5/UTF-8 ç·¨ç¢¼å•é¡Œ
    """
    bytes_data = uploaded_file.getvalue()
    
    # 1. å˜—è©¦å¸¸è¦‹ç·¨ç¢¼
    encodings = ['utf-8', 'cp950', 'big5', 'gbk', 'utf-16']
    
    for enc in encodings:
        try:
            # å˜—è©¦è§£ç¢¼ä¸¦æŒ‰è¡Œåˆ‡å‰²
            content = bytes_data.decode(enc)
            lines = content.splitlines()
            return lines
        except:
            continue
    return None

def clean_messy_erp_file(uploaded_file):
    """
    v8: ç´”æ–‡å­—è§£ææ¨¡å¼ï¼Œä¸ä¾è³´ Pandas çš„ read_csvï¼Œ
    å°ˆé–€å°ä»˜æ ¼å¼æ¥µåº¦æ··äº‚çš„ ERP å ±è¡¨ã€‚
    """
    lines = try_read_content(uploaded_file)
    
    if not lines:
        return None

    cleaned_rows = []
    current_date = None
    current_customer = None
    
    # é€è¡Œè§£æ
    for line in lines:
        # å»é™¤å¼•è™Ÿä¸­çš„é€—è™Ÿ (é¿å… CSV åˆ†å‰²éŒ¯èª¤)ï¼Œç°¡å–®è™•ç†
        # é€™è£¡å‡è¨­é‡‘é¡è£¡çš„é€—è™Ÿæ˜¯å¹²æ“¾æºï¼Œå…ˆç°¡å–®ç§»é™¤å¼•è™Ÿ
        line_clean = line.replace('"', '').replace("'", "")
        parts = line_clean.split(',')
        
        # ç§»é™¤å‰å¾Œç©ºç™½
        parts = [p.strip() for p in parts]
        
        # å¦‚æœåˆ‡å‡ºä¾†æ¬„ä½å¤ªå°‘ï¼Œå¯èƒ½æ˜¯ç©ºè¡Œ
        if len(parts) < 3: continue

        # 1. åµæ¸¬æ—¥æœŸè¡Œ
        # æª¢æŸ¥ç¬¬ 0 æ¬„æ˜¯å¦åŒ…å« "è¨‚å–®æ—¥æœŸ"
        if "è¨‚å–®æ—¥æœŸ" in parts[0]:
            # æ—¥æœŸé€šå¸¸åœ¨ç¬¬ 2 æˆ–ç¬¬ 3 å€‹ä½ç½®
            for p in parts[1:5]: 
                # ç°¡å–®æ­£å‰‡ï¼šæŠ“ 202x.xx.xx
                if re.search(r'202\d', p):
                    current_date = p.replace('.', '-').strip()
                    break
            continue

        # 2. åµæ¸¬è³‡æ–™è¡Œ
        # æ¢ä»¶ï¼šç¬¬ 5 æ¬„ (Index 5) æ˜¯ç”¢å“ç·¨è™Ÿï¼Œä¸”ä¸ç‚ºç©ºï¼Œä¸”ä¸æ˜¯æ¨™é¡Œ
        if len(parts) > 6:
            prod_id = parts[5]
            
            # éæ¿¾æ¢ä»¶
            if prod_id and prod_id != "ç”¢å“ç·¨è™Ÿ" and "åˆè¨ˆ" not in line and "ç¸½è¨ˆ" not in line:
                
                # æŠ“å®¢æˆ¶ (å¦‚æœç¬¬ 0 æ¬„æœ‰å­—ï¼Œå°±æ˜¯æ–°å®¢æˆ¶ï¼›æ²’å­—å°±æ²¿ç”¨èˆŠçš„)
                if parts[0]:
                    current_customer = parts[0]
                
                # æŠ“æ•¸é‡ (æœ€é›£çš„éƒ¨åˆ†)
                # ç­–ç•¥ï¼šå¾å¾Œé¢å¾€å‰æ‰¾ï¼Œæ‰¾åˆ°ã€Œå–®ä½ã€(MPS/KG/ç®±) ä¹‹å¾Œçš„æ•¸å­—
                
                qty = 0.0
                
                # å°‹æ‰¾å–®ä½çš„ä½ç½®
                unit_candidates = ["MPS", "KG", "PCS", "SET", "ç®±", "å°", "æ”¯", "ä¸€èˆ¬åŒ…è£"]
                unit_idx = -1
                
                # æƒæé€™ä¸€è¡Œï¼Œæ‰¾å–®ä½
                for idx, val in enumerate(parts):
                    if val in unit_candidates:
                        unit_idx = idx
                        break
                
                # å¦‚æœæ‰¾ä¸åˆ°å¸¸è¦‹å–®ä½ï¼Œå˜—è©¦æ‰¾ã€Œç”¢å“åç¨±ã€(Index 9) å¾Œé¢çš„éæ•¸å­—æ¬„ä½
                if unit_idx == -1 and len(parts) > 10:
                     for idx in range(10, len(parts)):
                         # æ‰¾ä¸€å€‹é•·åº¦çŸ­çš„éæ•¸å­—å­—ä¸²ç•¶ä½œå–®ä½
                         if parts[idx] and not parts[idx].replace('.','').isdigit() and len(parts[idx]) < 5:
                             unit_idx = idx
                             break
                
                # å¦‚æœæ‰¾åˆ°äº†å–®ä½ï¼Œæ•¸é‡é€šå¸¸åœ¨å–®ä½å¾Œé¢ 1~3 æ ¼å…§
                if unit_idx != -1:
                    potential_nums = []
                    for k in range(unit_idx + 1, min(unit_idx + 5, len(parts))):
                        val = parts[k].replace(',', '') # å»é™¤åƒåˆ†ä½
                        try:
                            f_val = float(val)
                            potential_nums.append(f_val)
                        except:
                            pass
                    
                    # é‚è¼¯ï¼šå¦‚æœæœ‰ 2 å€‹æ•¸å­—ï¼Œé€šå¸¸æ˜¯ [å–®åƒ¹, æ•¸é‡] -> å–ç¬¬ 2 å€‹
                    # å¦‚æœåªæœ‰ 1 å€‹æ•¸å­—ï¼Œå°±æ˜¯æ•¸é‡ -> å–ç¬¬ 1 å€‹
                    if len(potential_nums) >= 2:
                        qty = potential_nums[1]
                    elif len(potential_nums) == 1:
                        qty = potential_nums[0]
                
                # å¦‚æœé‚„æ˜¯æ²’æŠ“åˆ°ï¼Œå˜—è©¦ç›´æ¥æŠ“ç¬¬ 20~25 æ¬„ä½çš„æ•¸å­— (Blind guess)
                if qty == 0 and len(parts) > 20:
                     try:
                         # å˜—è©¦è®€å– prn34c.xls çµæ§‹ä¸­çš„æ•¸é‡ä½ç½®
                         candidate = parts[21].replace(',', '') # å‡è¨­ä½ç½®
                         if candidate: qty = float(candidate)
                     except:
                         pass

                if qty > 0:
                    cleaned_rows.append({
                        'å–®æ“šæ—¥æœŸ': current_date,
                        'å®¢æˆ¶åç¨±': current_customer,
                        'ç”¢å“ç·¨è™Ÿ': prod_id,
                        'æ•¸é‡': qty
                    })

    if not cleaned_rows:
        return None
        
    return pd.DataFrame(cleaned_rows)

# ==========================================
# ğŸ“¦ è¼”åŠ©åŠŸèƒ½
# ==========================================
def generate_example_file():
    output = io.BytesIO()
    data = {
        'å–®æ“šæ—¥æœŸ': ['2023.01.15', '2023.02.20'],
        'å®¢æˆ¶åç¨±': ['å®¢æˆ¶A', 'å®¢æˆ¶A'],
        'ç”¢å“ç·¨è™Ÿ': ['P001', 'P001'],
        'æ•¸é‡': [100, 150]
    }
    df = pd.DataFrame(data)
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False)
    output.seek(0)
    return output.getvalue()

def convert_df_to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='é æ¸¬çµæœ')
        try:
            worksheet = writer.sheets['é æ¸¬çµæœ']
            for i, col in enumerate(df.columns):
                col_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
                worksheet.set_column(i, i, col_len)
        except:
            pass
    output.seek(0)
    return output.getvalue()

# ==========================================
# ğŸ” è³‡æ–™é æª¢èˆ‡çµæ§‹åŒ–
# ==========================================
def audit_and_process_data(uploaded_file):
    # å˜—è©¦è®€å–
    processed_dict = {}
    audit_info = {"total_rows": 0, "detected_columns": {}, "grouping_mode": "æœªçŸ¥", "groups_found": 0}
    
    # 1. å…ˆå˜—è©¦æ¨™æº–è®€å–
    raw_sheets = {}
    try:
        raw_sheets = pd.read_excel(uploaded_file, sheet_name=None)
    except:
        pass # å¤±æ•—ä¹Ÿæ²’é—œä¿‚ï¼Œå¾Œé¢æœƒè™•ç†

    # 2. åˆ¤æ–·æ˜¯å¦éœ€è¦æ¸…æ´—
    needs_cleaning = True
    if raw_sheets:
        for _, df in raw_sheets.items():
            if find_column(df, 'date') and find_column(df, 'qty'):
                needs_cleaning = False # æœ‰æ¨™æº–æ¬„ä½ï¼Œä¸ç”¨æ´—
                break
    
    if needs_cleaning:
        uploaded_file.seek(0)
        # st.toast("å•Ÿå‹•å¼·åŠ›æ¸…æ´—æ¨¡å¼ (Big5/UTF-8)...", icon="ğŸ§¹")
        df_cleaned = clean_messy_erp_file(uploaded_file)
        
        if df_cleaned is not None and not df_cleaned.empty:
            raw_sheets = {'Cleaned_Data': df_cleaned}
        else:
            return "âŒ æª”æ¡ˆè®€å–å¤±æ•—ã€‚è«‹ç¢ºèªæª”æ¡ˆä¸æ˜¯æå£çš„ï¼Œæˆ–å˜—è©¦å°‡æª”æ¡ˆå¦å­˜ç‚ºæ¨™æº– CSV (UTF-8) æ ¼å¼ã€‚", None, None

    # --- ä»¥ä¸‹æ¨™æº–åŒ–æµç¨‹ ---
    for sheet_name, df in raw_sheets.items():
        if df.empty: continue
        
        col_date = find_column(df, 'date')
        col_qty = find_column(df, 'qty')
        col_prod = find_column(df, 'product')
        col_cust = find_column(df, 'customer')
        
        if not col_date or not col_qty: continue
            
        audit_info["detected_columns"] = {"æ—¥æœŸ": col_date, "æ•¸é‡": col_qty, "ç”¢å“": col_prod, "å®¢æˆ¶": col_cust}
        
        rename_map = {col_date: 'date', col_qty: 'æ•¸é‡'}
        if col_prod: rename_map[col_prod] = 'product_id'
        if col_cust: rename_map[col_cust] = 'customer_id'
        df = df.rename(columns=rename_map)
        
        if col_prod and col_cust:
            audit_info["grouping_mode"] = "ç²¾æº–æ¨¡å¼ (å®¢æˆ¶ + ç”¢å“)"
            for (cid, pid), sub_df in df.groupby(['customer_id', 'product_id']):
                key = (str(cid).strip(), str(pid).strip())
                processed_dict[key] = sub_df
        elif col_prod:
            audit_info["grouping_mode"] = "ç”¢å“æ¨¡å¼"
            for pid, sub_df in df.groupby('product_id'):
                key = ("å…¨éƒ¨å®¢æˆ¶", str(pid).strip())
                processed_dict[key] = sub_df
        else:
            audit_info["grouping_mode"] = "ç°¡æ˜“æ¨¡å¼"
            key = ("é è¨­", sheet_name)
            processed_dict[key] = df
            
        audit_info["total_rows"] += len(df)

    audit_info["groups_found"] = len(processed_dict)
    if not processed_dict:
        return "âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆè³‡æ–™", None, None
        
    return "OK", processed_dict, audit_info

# ==========================================
# ğŸ¤– é æ¸¬å¼•æ“
# ==========================================
def run_prediction_engine(processed_data):
    final_summary = []
    progress_bar = st.progress(0)
    total = len(processed_data)
    count = 0

    for (cust_id, prod_id), df in processed_data.items():
        count += 1
        progress_bar.progress(int((count / total) * 100))

        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        df['æ•¸é‡'] = pd.to_numeric(df['æ•¸é‡'], errors='coerce')
        df = df[df['æ•¸é‡'] > 0].dropna(subset=['date', 'æ•¸é‡']).sort_values('date').reset_index(drop=True)
        if df.empty: continue

        df['temp_gap'] = df['date'].diff().dt.days.fillna(999)
        df['session_id'] = (df['temp_gap'] > 7).cumsum()
        df = df.groupby('session_id').agg({'date': 'last', 'æ•¸é‡': 'sum'}).reset_index(drop=True)
        if len(df) < 2: continue

        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['days_since_last'] = df['date'].diff().dt.days.fillna(0)
        df['target_days'] = df['date'].shift(-1).diff().dt.days.shift(-1)
        df['target_qty'] = df['æ•¸é‡'].shift(-1)
        
        train_df = df.dropna(subset=['target_days', 'target_qty']).copy()
        if len(train_df) >= 3:
            train_df = train_df[train_df['year'] >= 2022]
            if len(train_df) < 3: train_df = df.tail(10)

        last_row = df.tail(1).copy()
        
        if len(train_df) < 5:
            p_days = df['days_since_last'].median()
            p_qty = df['æ•¸é‡'].median()
            conf = "ä½ (çµ±è¨ˆ)"
        else:
            try:
                model_d = RandomForestRegressor(n_estimators=100, random_state=42)
                model_d.fit(train_df[['æ•¸é‡', 'days_since_last', 'month']], train_df['target_days'])
                p_days = model_d.predict(last_row[['æ•¸é‡', 'days_since_last', 'month']])[0]
                p_qty = df['æ•¸é‡'].median()
                conf = "é«˜ (AI)"
            except:
                p_days = df['days_since_last'].median()
                p_qty = df['æ•¸é‡'].median()
                conf = "ä½ (éŒ¯èª¤)"

        p_days = max(1, int(p_days))
        date_1 = last_row['date'].iloc[0] + timedelta(days=p_days)
        
        final_summary.append({
            'å®¢æˆ¶åç¨±': cust_id, 'ç”¢å“ç·¨è™Ÿ': prod_id, 'åˆ†æä¿¡å¿ƒåº¦': conf,
            'æœ€å¾Œä¸‹å–®æ—¥': last_row['date'].iloc[0].strftime('%Y-%m-%d'),
            'ã€é æ¸¬1ã€‘æ—¥æœŸ': date_1.strftime('%Y-%m-%d'),
            'ã€é æ¸¬1ã€‘æ•¸é‡': int(p_qty),
            'æ­·å²è¨‚å–®æ•¸': len(df)
        })

    progress_bar.empty()
    if final_summary: return pd.DataFrame(final_summary)
    return None

# ==========================================
# ğŸ–¥ï¸ ç¶²é ä¸»ä»‹é¢
# ==========================================
def main():
    st.title("ğŸ§  CRM æ™ºæ…§ç”¢å“é æ¸¬ç³»çµ± (v8)")
    st.caption("æ”¯æ´åŠŸèƒ½ï¼šå¼·åŠ›æ¸…æ´— ERP äº‚ç¢¼å ±è¡¨ â€¢ å®¢æˆ¶åˆ†ç¾¤é æ¸¬ â€¢ æ™ºæ…§æ¬„ä½åµæ¸¬")

    with st.sidebar:
        st.header("1. æº–å‚™è³‡æ–™")
        ex_file = generate_example_file()
        st.download_button("ğŸ“¥ ä¸‹è¼‰æ¨™æº–ç¯„æœ¬", ex_file, "template.xlsx")

    st.header("2. ä¸Šå‚³èˆ‡æª¢æ ¸")
    uploaded_file = st.file_uploader("ä¸Šå‚³ Excel/CSV (æ”¯æ´ prn/txt åŒ¯å‡ºæª”)", type=['xlsx', 'csv', 'xls', 'txt'])

    if uploaded_file:
        status, processed_data, audit_info = audit_and_process_data(uploaded_file)

        if status != "OK":
            st.error(status)
            st.warning("ğŸ’¡ æç¤ºï¼šå¦‚æœä¾ç„¶ç„¡æ³•è®€å–ï¼Œè«‹å°‡è©²æª”æ¡ˆåœ¨ Excel ä¸­é–‹å•Ÿï¼Œä¸¦ã€å¦å­˜æ–°æª”ã€ç‚º CSV (UTF-8) æ ¼å¼å¾Œå†ä¸Šå‚³ã€‚")
        else:
            st.success("âœ… æª”æ¡ˆè®€å–æˆåŠŸï¼")
            
            st.subheader("ğŸ§ è³‡æ–™é è¦½ (è«‹ç¢ºèªæ•¸é‡æ˜¯å¦æ­£ç¢º)")
            
            # æŠ“å‡ºå‰ 5 ç­†é è¦½
            if processed_data:
                preview_list = []
                for k, df in list(processed_data.items())[:5]:
                    temp = df.head(2).copy()
                    temp['Group_Key'] = str(k)
                    preview_list.append(temp)
                if preview_list:
                    preview_df = pd.concat(preview_list)
                    st.dataframe(preview_df.head(10), use_container_width=True)
            
            st.info(f"åµæ¸¬åˆ° {audit_info['groups_found']} çµ„ç”¢å“ï¼Œå…± {audit_info['total_rows']} ç­†äº¤æ˜“ã€‚")

            if st.button("ğŸš€ ç¢ºèªç„¡èª¤ï¼Œé–‹å§‹é æ¸¬", type="primary"):
                result_df = run_prediction_engine(processed_data)
                if result_df is not None:
                    st.divider()
                    st.success(f"å®Œæˆï¼å…±ç”¢å‡º {len(result_df)} ç­†é æ¸¬ã€‚")
                    st.dataframe(result_df, use_container_width=True)
                    excel_data = convert_df_to_excel(result_df)
                    st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š", excel_data, "prediction_v8.xlsx")
                else:
                    st.warning("âš ï¸ ç„¡æ³•ç”¢å‡ºçµæœ (æ­·å²è³‡æ–™ä¸è¶³)ã€‚")

if __name__ == "__main__":
    main()