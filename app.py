import streamlit as st
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from datetime import timedelta
import io

# --- ç¶²é è¨­å®š ---
st.set_page_config(page_title="CRM æ™ºæ…§ç”¢å“é æ¸¬ç³»çµ±", page_icon="ğŸ§ ", layout="wide")

# ==========================================
# ğŸ§¹ ERP è³‡æ–™æ¸…æ´—æ ¸å¿ƒ (æ–°å¢åŠŸèƒ½)
# ==========================================
def clean_messy_erp_file(uploaded_file):
    """
    å°ˆé–€è™•ç†æ ¼å¼è·‘æ‰çš„ ERP å ±è¡¨ (åˆ—å°æ ¼å¼è½‰ Excel)
    é‚è¼¯ï¼š
    1. è‡ªå‹•å¡«è£œä¸Šæ–¹æ—¥æœŸçš„éšå±¤
    2. è‡ªå‹•å¡«è£œå®¢æˆ¶åç¨±
    3. æ ¹æ“š 'å–®ä½' æ¬„ä½å®šä½ï¼Œå‹•æ…‹æŠ“å–å¾Œæ–¹çš„å–®åƒ¹èˆ‡æ•¸é‡
    """
    try:
        # ä½¿ç”¨ header=None è®€å–ï¼Œé¿å…æ¨™é¡Œè¢«åƒæ‰
        df_raw = pd.read_csv(uploaded_file, header=None)
    except:
        try:
            df_raw = pd.read_excel(uploaded_file, header=None)
        except:
            return None

    cleaned_rows = []
    current_date = None
    current_customer = None
    
    # ç¡¬ç·¨ç¢¼é—œéµæ¬„ä½ä½ç½® (åŸºæ–¼ prn34c.xls åˆ†æ)
    # ç”¢å“ç·¨è™Ÿé€šå¸¸åœ¨ç¬¬ 5 æ¬„ (Index 5)
    # ç”¢å“åç¨±é€šå¸¸åœ¨ç¬¬ 9 æ¬„ (Index 9)
    
    for i, row in df_raw.iterrows():
        # 1. åµæ¸¬æ—¥æœŸè¡Œ (ä¾‹å¦‚: "è¨‚å–®æ—¥æœŸ  2026.01.02")
        first_col = str(row[0]).strip()
        if "è¨‚å–®æ—¥æœŸ" in first_col:
            val = str(row[2]).strip() # æ—¥æœŸé€šå¸¸åœ¨ç¬¬ 3 æ¬„
            if val and val != 'nan':
                current_date = val.replace('.', '-') # è½‰æˆæ¨™æº–æ ¼å¼
            continue

        # 2. åµæ¸¬è³‡æ–™è¡Œ (ç”¢å“ç·¨è™Ÿå­˜åœ¨ä¸”ä¸æ˜¯æ¨™é¡Œ)
        prod_id = str(row[5]).strip()
        if prod_id and prod_id != 'nan' and prod_id != "ç”¢å“ç·¨è™Ÿ":
            
            # è™•ç†å®¢æˆ¶ (å¦‚æœç©ºç™½å°±ç”¨ä¸Šä¸€å€‹)
            cust = str(row[0]).strip()
            if cust and cust != 'nan':
                current_customer = cust
            
            # è™•ç†æ•¸é‡èˆ‡å–®åƒ¹ (æœ€å›°é›£çš„éƒ¨åˆ†ï¼šæ¬„ä½æœƒä½ç§»)
            # ç­–ç•¥ï¼šæ‰¾åˆ°ã€Œç”¢å“åç¨±ã€å¾Œé¢çš„ã€Œå–®ä½ã€æ¬„ä½ï¼Œæ•¸å€¼é€šå¸¸åœ¨å–®ä½å¾Œé¢
            unit_idx = -1
            prod_name_col = 9
            
            # å¾€å¾Œæ‰¾ã€Œå–®ä½ã€(é€šå¸¸æ˜¯æ–‡å­—ä¸”é•·åº¦çŸ­)
            for c in range(prod_name_col + 1, len(row)):
                val = str(row[c]).strip()
                # åˆ¤æ–·æ˜¯å¦ç‚ºå–®ä½ (éæ•¸å­—, éç©º)
                try:
                    float(val.replace(',', ''))
                    is_num = True
                except:
                    is_num = False
                
                if val and val != 'nan' and not is_num:
                    unit_idx = c
                    break
            
            qty = 0.0
            price = 0.0
            
            if unit_idx != -1:
                # æ”¶é›†å–®ä½å¾Œé¢çš„æ‰€æœ‰æ•¸å­—
                nums = []
                for c in range(unit_idx + 1, len(row)):
                    val = str(row[c]).strip()
                    try:
                        num = float(val.replace(',', ''))
                        nums.append(num)
                    except:
                        pass
                    if len(nums) >= 3: break # é€šå¸¸åªè¦å–®åƒ¹ã€æ•¸é‡ã€é‡‘é¡
                
                # å•Ÿç™¼å¼è¦å‰‡ï¼š
                # å¦‚æœæœ‰ 3 å€‹æ•¸å­— -> [å–®åƒ¹, æ•¸é‡, é‡‘é¡]
                # å¦‚æœæœ‰ 2 å€‹æ•¸å­— -> [å–®åƒ¹, æ•¸é‡] (æˆ–æ˜¯ [æ•¸é‡, é‡‘é¡]?)
                if len(nums) >= 2:
                    price = nums[0]
                    qty = nums[1]
                elif len(nums) == 1:
                    qty = nums[0] # åªæœ‰ä¸€å€‹æ•¸å­—é€šå¸¸æ˜¯æ•¸é‡
            
            # æ’é™¤åˆè¨ˆè¡Œ
            if "åˆè¨ˆ" not in str(row.values) and "ç¸½è¨ˆ" not in str(row.values):
                cleaned_rows.append({
                    'å–®æ“šæ—¥æœŸ': current_date,
                    'å®¢æˆ¶åç¨±': current_customer,
                    'ç”¢å“ç·¨è™Ÿ': prod_id, # é€™è£¡å°æ‡‰ v6 çš„ product æ¬„ä½
                    'æ•¸é‡': qty
                })

    if not cleaned_rows:
        return None
        
    return pd.DataFrame(cleaned_rows)

# ==========================================
# ğŸ§  æ ¸å¿ƒå‡ç´š: æ™ºæ…§æ¬„ä½å°ç…§è¡¨ (ç¶­æŒ v6)
# ==========================================
COLUMN_MAPPING = {
    'date': ['å–®æ“šæ—¥æœŸ', 'ä¸‹å–®æ—¥', 'æ—¥æœŸ', 'Date', 'Order Date'],
    'qty': ['æ•¸é‡', 'è¨‚å–®æ•¸é‡', 'éŠ·è²¨æ•¸é‡', 'Qty', 'Quantity'],
    'product': ['ç”¢å“ç·¨è™Ÿ', 'å“è™Ÿ', 'å“å', 'æ–™è™Ÿ', 'Product ID'],
    'customer': ['å®¢æˆ¶', 'å®¢æˆ¶ä»£è™Ÿ', 'å®¢æˆ¶ç°¡ç¨±', 'å®¢æˆ¶åç¨±']
}

def find_column(df, target_type):
    """æ™ºæ…§å°‹æ‰¾æ¬„ä½åç¨±"""
    candidates = COLUMN_MAPPING.get(target_type, [])
    for col in df.columns:
        if str(col).strip() in candidates:
            return col
    for col in df.columns:
        for candidate in candidates:
            if candidate in str(col):
                return col
    return None

# ... (generate_example_file, convert_df_to_excel ç¶­æŒä¸è®Šï¼Œçœç•¥ä»¥ç¯€çœç¯‡å¹…) ...
def generate_example_file():
    output = io.BytesIO()
    data = {
        'å®¢æˆ¶ä»£è™Ÿ': ['C001', 'C001'],
        'ç”¢å“ç·¨è™Ÿ': ['P-1001', 'P-1001'],
        'å–®æ“šæ—¥æœŸ': ['2023.01.15', '2023.02.20'],
        'æ•¸é‡': [100, 150]
    }
    df = pd.DataFrame(data)
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False)
    output.seek(0)
    return output.getvalue()

def convert_df_to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False)
    output.seek(0)
    return output.getvalue()

# ==========================================
# ğŸ” è³‡æ–™é æª¢èˆ‡çµæ§‹åŒ– (æ•´åˆæ¸…æ´—é‚è¼¯)
# ==========================================
def audit_and_process_data(uploaded_file):
    # 1. å˜—è©¦ç›´æ¥è®€å– (æ¨™æº– Excel)
    try:
        raw_sheets = pd.read_excel(uploaded_file, sheet_name=None)
    except:
        # å¦‚æœè®€å¤±æ•—ï¼Œå¯èƒ½æ˜¯ CSV æˆ–äº‚ç¢¼æª”
        uploaded_file.seek(0)
        raw_sheets = {'Sheet1': pd.read_csv(uploaded_file)}

    processed_dict = {}
    audit_info = {"total_rows": 0, "detected_columns": {}, "grouping_mode": "æœªçŸ¥", "groups_found": 0}
    
    # ğŸš© åˆ¤æ–·æ˜¯å¦éœ€è¦å•Ÿå‹•ã€ŒERP æ¸…æ´—æ¨¡å¼ã€
    # å¦‚æœè®€é€²ä¾†ç¬¬ä¸€æ¬„æœ‰å¾ˆå¤š NaNï¼Œæˆ–è€…æ‰¾ä¸åˆ°æ¨™é¡Œï¼Œå¾ˆæœ‰å¯èƒ½æ˜¯è·‘æ‰çš„æ ¼å¼
    needs_cleaning = False
    for _, df in raw_sheets.items():
        if find_column(df, 'date') is None and find_column(df, 'qty') is None:
            needs_cleaning = True
            break
    
    if needs_cleaning:
        uploaded_file.seek(0)
        st.toast("åµæ¸¬åˆ°éæ¨™æº–æ ¼å¼ï¼Œæ­£åœ¨å•Ÿå‹• ERP æ¸…æ´—å¼•æ“...", icon="ğŸ§¹")
        df_cleaned = clean_messy_erp_file(uploaded_file)
        
        if df_cleaned is not None and not df_cleaned.empty:
            # æ¸…æ´—æˆåŠŸï¼Œå°‡å…¶è¦–ç‚ºæ¨™æº–è³‡æ–™ç¹¼çºŒè™•ç†
            raw_sheets = {'Cleaned_Data': df_cleaned}
        else:
            return "âŒ ç„¡æ³•è‡ªå‹•æ¸…æ´—æ­¤æª”æ¡ˆï¼Œè«‹æª¢æŸ¥æ ¼å¼ã€‚", None, None

    # --- ä»¥ä¸‹é‚è¼¯èˆ‡ v6 ç›¸åŒ (æ¨™æº–åŒ–è™•ç†) ---
    for sheet_name, df in raw_sheets.items():
        if df.empty: continue
        
        col_date = find_column(df, 'date')
        col_qty = find_column(df, 'qty')
        col_prod = find_column(df, 'product')
        col_cust = find_column(df, 'customer')
        
        if not col_date or not col_qty: continue
            
        audit_info["detected_columns"] = {
            "æ—¥æœŸ": col_date, "æ•¸é‡": col_qty, 
            "ç”¢å“": col_prod, "å®¢æˆ¶": col_cust
        }
        
        rename_map = {col_date: 'date', col_qty: 'æ•¸é‡'}
        if col_prod: rename_map[col_prod] = 'product_id'
        if col_cust: rename_map[col_cust] = 'customer_id'
        df = df.rename(columns=rename_map)
        
        # åˆ†çµ„é‚è¼¯
        if col_prod and col_cust:
            audit_info["grouping_mode"] = "ç²¾æº–æ¨¡å¼ (å®¢æˆ¶ + ç”¢å“)"
            for (cid, pid), sub_df in df.groupby(['customer_id', 'product_id']):
                key = (str(cid).strip(), str(pid).strip())
                processed_dict[key] = sub_df
        elif col_prod:
            audit_info["grouping_mode"] = "ç”¢å“æ¨¡å¼"
            for pid, sub_df in df.groupby('product_id'):
                key = ("å…¨éƒ¨å®¢æˆ¶", str(pid).strip())
                processed_dict[key] = sub_df
        else:
            audit_info["grouping_mode"] = "ç°¡æ˜“æ¨¡å¼"
            key = ("é è¨­", sheet_name)
            processed_dict[key] = df
            
        audit_info["total_rows"] += len(df)

    audit_info["groups_found"] = len(processed_dict)
    if not processed_dict:
        return "âŒ æ‰¾ä¸åˆ°æœ‰æ•ˆè³‡æ–™", None, None
        
    return "OK", processed_dict, audit_info

# ... (run_prediction_engine ç¶­æŒ v6 ä¸è®Šï¼Œçœç•¥) ...
def run_prediction_engine(processed_data):
    final_summary = []
    progress_bar = st.progress(0)
    total = len(processed_data)
    count = 0

    for (cust_id, prod_id), df in processed_data.items():
        count += 1
        progress_bar.progress(int((count / total) * 100))

        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        df['æ•¸é‡'] = pd.to_numeric(df['æ•¸é‡'], errors='coerce')
        df = df[df['æ•¸é‡'] > 0].dropna(subset=['date', 'æ•¸é‡']).sort_values('date').reset_index(drop=True)
        if df.empty: continue

        df['temp_gap'] = df['date'].diff().dt.days.fillna(999)
        df['session_id'] = (df['temp_gap'] > 7).cumsum()
        df = df.groupby('session_id').agg({'date': 'last', 'æ•¸é‡': 'sum'}).reset_index(drop=True)
        if len(df) < 2: continue

        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['days_since_last'] = df['date'].diff().dt.days.fillna(0)
        df['target_days'] = df['date'].shift(-1).diff().dt.days.shift(-1)
        df['target_qty'] = df['æ•¸é‡'].shift(-1)
        
        train_df = df.dropna(subset=['target_days', 'target_qty']).copy()
        if len(train_df) >= 3:
            train_df = train_df[train_df['year'] >= 2022]
            if len(train_df) < 3: train_df = df.tail(10)

        last_row = df.tail(1).copy()
        
        # æ··åˆé æ¸¬
        if len(train_df) < 5:
            p_days = df['days_since_last'].median()
            p_qty = df['æ•¸é‡'].median()
            conf = "ä½ (çµ±è¨ˆ)"
        else:
            try:
                model_d = RandomForestRegressor(n_estimators=100, random_state=42)
                model_d.fit(train_df[['æ•¸é‡', 'days_since_last', 'month']], train_df['target_days'])
                p_days = model_d.predict(last_row[['æ•¸é‡', 'days_since_last', 'month']])[0]
                p_qty = df['æ•¸é‡'].median() # ç°¡åŒ–æ•¸é‡é æ¸¬ä»¥æ±‚ç©©
                conf = "é«˜ (AI)"
            except:
                p_days = df['days_since_last'].median()
                p_qty = df['æ•¸é‡'].median()
                conf = "ä½ (éŒ¯èª¤)"

        p_days = max(1, int(p_days))
        date_1 = last_row['date'].iloc[0] + timedelta(days=p_days)
        
        final_summary.append({
            'å®¢æˆ¶åç¨±': cust_id, 'ç”¢å“ç·¨è™Ÿ': prod_id, 'åˆ†æä¿¡å¿ƒåº¦': conf,
            'æœ€å¾Œä¸‹å–®æ—¥': last_row['date'].iloc[0].strftime('%Y-%m-%d'),
            'ã€é æ¸¬1ã€‘æ—¥æœŸ': date_1.strftime('%Y-%m-%d'),
            'ã€é æ¸¬1ã€‘æ•¸é‡': int(p_qty)
        })

    progress_bar.empty()
    if final_summary: return pd.DataFrame(final_summary)
    return None

# ==========================================
# ğŸ–¥ï¸ ç¶²é ä¸»ä»‹é¢
# ==========================================
def main():
    st.title("ğŸ§  CRM æ™ºæ…§ç”¢å“é æ¸¬ç³»çµ± (v7)")
    st.caption("æ–°å¢åŠŸèƒ½ï¼šè‡ªå‹•æ¸…æ´— ERP åˆ—å°æ ¼å¼å ±è¡¨ (Prn/Excel)")

    with st.sidebar:
        st.header("1. æº–å‚™è³‡æ–™")
        ex_file = generate_example_file()
        st.download_button("ğŸ“¥ ä¸‹è¼‰æ¨™æº–ç¯„æœ¬", ex_file, "template.xlsx")

    st.header("2. ä¸Šå‚³èˆ‡æª¢æ ¸")
    uploaded_file = st.file_uploader("ä¸Šå‚³ Excel/CSV (æ”¯æ´æ¨™æº–æ ¼å¼æˆ– ERP åŒ¯å‡º)", type=['xlsx', 'csv', 'xls'])

    if uploaded_file:
        status, processed_data, audit_info = audit_and_process_data(uploaded_file)

        if status != "OK":
            st.error(status)
        else:
            st.success("âœ… æª”æ¡ˆè®€å–æˆåŠŸï¼")
            
            # --- æ–°å¢ï¼šè³‡æ–™é è¦½å€ (è®“æ‚¨å¯©æ ¸æ•¸å€¼) ---
            st.subheader("ğŸ§ è³‡æ–™æ¸…æ´—çµæœé è¦½")
            st.markdown("è«‹æª¢æŸ¥ä¸‹æ–¹çš„**ã€æ•¸é‡ã€‘**èˆ‡**ã€ç”¢å“ã€‘**æ˜¯å¦æ­£ç¢ºæŠ“å–ï¼š")
            
            # æŠ“å‡ºå‰ 5 ç­†é è¦½
            preview_df = pd.concat([df.head(2) for k, df in list(processed_data.items())[:3]])
            st.dataframe(preview_df, use_container_width=True)
            
            st.info(f"åµæ¸¬åˆ° {audit_info['groups_found']} çµ„åˆ†æå°è±¡ï¼Œå…± {audit_info['total_rows']} ç­†äº¤æ˜“ã€‚")

            if st.button("ğŸš€ ç¢ºèªç„¡èª¤ï¼Œé–‹å§‹é æ¸¬", type="primary"):
                result_df = run_prediction_engine(processed_data)
                if result_df is not None:
                    st.divider()
                    st.success(f"å®Œæˆï¼å…±ç”¢å‡º {len(result_df)} ç­†é æ¸¬ã€‚")
                    st.dataframe(result_df, use_container_width=True)
                    excel_data = convert_df_to_excel(result_df)
                    st.download_button("ğŸ“¥ ä¸‹è¼‰å ±å‘Š", excel_data, "prediction_v7.xlsx")

if __name__ == "__main__":
    main()